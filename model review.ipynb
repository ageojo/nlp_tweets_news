{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Goal of machine learning: what class does a particular datum belong to?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each tweet is a *document* and each word/token is a *feature*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so given a labeled document, the algorithm tries to understand which features correspond to the label (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define $C_i$ to be the ith class/category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define p(x) to the be the probability of observing event x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for document classification, both positive and negative sentiment labels would define the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "likewise, words or tokens would be events, and p(x) would be the associated probability of observing a particular word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|x) = \\frac{p(x|c)p(c)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(c|x)$ = probability of event x being in class c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x|c)$ = probability of *generating* event x given class c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(c)$ = probability of occurance of class c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(d)$ = probability of instance d occuring (usually a pain in the ass to calculate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$posterior = \\frac{likelihood * prior}{evidence}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(evidence is independent of C, hence irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|x) = \\frac{p(x|c)p(c)}{p(x)} \\sim p(x|c)p(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c_i|x,y) > p(c_j|x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose the class that maximizes the log posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typically used when there are lots of features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|F_1, F_2, .., F_n) = \\frac{p(F_1,F_2,..,F_n|c)p(c)}{p(F_1,F_2,..,F_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definition of conditional probability*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(A|B) = \\frac{p(A \\cap B)}{p(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ => p(F_1,F_2,..,F_n,c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Chain Rule for conditional probability*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A_4,A_3,A_2,A_1) = P(A_4|A_3,A_2,A_1) * P(A_3|A_2,A_1) * P(A_2,A_1) * P(A_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(F_1,F_2,..,F_n,c) \\propto p(c) p(F_1|c) p(F_2|c,F_1) \\ldots p(F_n|c,F_1,F_2,F_3,\\ldots,F_{n-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditional independence - given a class, assume the features are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(F_2|c,F_1)=p(F_2|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(F_n|c,F_1,F_2,F_3,…,F_{n−1}) = p(F_n|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|F_1,F_2,F_3,…,F_n) \\propto p(c) \\Pi p(F_i|c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Implementations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli document model - a document is represented by a feature vector with *binary* elements {0,1} indicating the presence or absense of a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial document model - a document is represented by a feature vector with integer elements who value is the frequency of that word in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoothing - need to generalize the results to avoid sample-bias - use Laplace smoothing (ad-hoc nonsense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logs - take logs to avoid underflow issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents -> bag of words + ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BBB = beginning of sentence marker\n",
    "# EEE = end of sentence marker\n",
    "tweets = [\"BBB a document is represented by a feature vector EEE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bbb', u'by', u'document', u'eee', u'feature', u'is', u'represented', u'vector']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vectorizer = CountVectorizer(min_df=1)\n",
    "X1 = unigram_vectorizer.fit_transform(tweets)\n",
    "print unigram_vectorizer.get_feature_names()\n",
    "print\n",
    "X1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'a document', u'a feature', u'bbb', u'bbb a', u'by', u'by a', u'document', u'document is', u'eee', u'feature', u'feature vector', u'is', u'is represented', u'represented', u'represented by', u'vector', u'vector eee']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X2 = bigram_vectorizer.fit_transform(tweets)\n",
    "print bigram_vectorizer.get_feature_names()\n",
    "print\n",
    "X2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'a document', u'a document is', u'a feature', u'a feature vector', u'bbb', u'bbb a', u'bbb a document', u'by', u'by a', u'by a feature', u'document', u'document is', u'document is represented', u'eee', u'feature', u'feature vector', u'feature vector eee', u'is', u'is represented', u'is represented by', u'represented', u'represented by', u'represented by a', u'vector', u'vector eee']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X3 = trigram_vectorizer.fit_transform(tweets)\n",
    "print trigram_vectorizer.get_feature_names()\n",
    "print\n",
    "X3.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"BBB a document is represented by a feature vector EEE\",\n",
    "    \"BBB let's choose a sentence with a completely different set of words (features) so we can waste lots of memory EEE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'a completely', u'a completely different', u'a document', u'a document is', u'a feature', u'a feature vector', u'a sentence', u'a sentence with', u'bbb', u'bbb a', u'bbb a document', u'bbb let', u'bbb let s', u'by', u'by a', u'by a feature', u'can', u'can waste', u'can waste lots', u'choose', u'choose a', u'choose a sentence', u'completely', u'completely different', u'completely different set', u'different', u'different set', u'different set of', u'document', u'document is', u'document is represented', u'eee', u'feature', u'feature vector', u'feature vector eee', u'features', u'features so', u'features so we', u'is', u'is represented', u'is represented by', u'let', u'let s', u'let s choose', u'lots', u'lots of', u'lots of memory', u'memory', u'memory eee', u'of', u'of memory', u'of memory eee', u'of words', u'of words features', u'represented', u'represented by', u'represented by a', u's', u's choose', u's choose a', u'sentence', u'sentence with', u'sentence with a', u'set', u'set of', u'set of words', u'so', u'so we', u'so we can', u'vector', u'vector eee', u'waste', u'waste lots', u'waste lots of', u'we', u'we can', u'we can waste', u'with', u'with a', u'with a completely', u'words', u'words features', u'words features so']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X3 = trigram_vectorizer.fit_transform(tweets)\n",
    "print trigram_vectorizer.get_feature_names()\n",
    "print\n",
    "X3.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression + L1 regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
